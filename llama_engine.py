"""
LLaMA ã‚¨ãƒ³ã‚¸ãƒ³ - llama.cpp ã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
"""

import os
from typing import Optional
from llama_cpp import Llama

class LlamaEngine:
    def __init__(self):
        self.model = None
        self.model_path = None
        self._initialize_model()
    
    def _initialize_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–"""
        try:
            # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’è¨­å®š
            model_file = "models/DeepSeek-R1-Distill-Qwen-14B-Japanese-Q4_K_M.gguf"
            
            if not os.path.exists(model_file):
                print(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_file}")
                return
            
            print(f"ğŸ”„ ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {model_file}")
            print("ğŸ“ ã“ã‚Œã«ã¯æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™...")
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
            self.model = Llama(
                model_path=model_file,
                n_ctx=2048,  # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·
                n_threads=4,  # ã‚¹ãƒ¬ãƒƒãƒ‰æ•°
                verbose=False,  # è©³ç´°ãƒ­ã‚°ã‚’ç„¡åŠ¹åŒ–
                n_gpu_layers=0  # GPUä½¿ç”¨ã—ãªã„ï¼ˆCPUã®ã¿ï¼‰
            )
            
            self.model_path = model_file
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {model_file}")
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            self.model = None
            self.model_path = None
    
    def is_ready(self) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ãŒä½¿ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
        return self.model is not None
    
    def generate(self, prompt: str, max_tokens: int = 512, temperature: float = 0.7, 
                top_p: float = 0.9, stop: Optional[list] = None) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
        if not self.is_ready():
            raise RuntimeError("ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ—¥æœ¬èªå¯¾å¿œã§èª¿æ•´
            formatted_prompt = f"ä»¥ä¸‹ã®æŒ‡ç¤ºã«å¾“ã£ã¦ã€æ—¥æœ¬èªã§å‰µä½œçš„ãªãƒ—ãƒ­ãƒƒãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n\n{prompt}\n\nå›ç­”:"
            
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            response = self.model(
                formatted_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                stop=stop or ["</s>", "<|endoftext|>", "\n\n---"],
                echo=False  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å«ã‚ãªã„
            )
            
            # ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            generated_text = response['choices'][0]['text'].strip()
            
            # ä¸è¦ãªéƒ¨åˆ†ã‚’é™¤å»
            if "å›ç­”:" in generated_text:
                generated_text = generated_text.split("å›ç­”:")[-1].strip()
            
            return generated_text
            
        except Exception as e:
            raise RuntimeError(f"ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_model_info(self) -> dict:
        """ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—"""
        return {
            "model_path": self.model_path,
            "is_ready": self.is_ready(),
            "model_type": "DeepSeek-R1-Distill-Qwen-14B-Japanese-Q4_K_M"
        }

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
llama_engine = LlamaEngine() 